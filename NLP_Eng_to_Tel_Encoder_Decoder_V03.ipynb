{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UnvR04Po7A5O",
        "0rS9TfEn7FVG",
        "edRRo0zCPseS"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chandanareddy-enugala/NLP-SLU/blob/main/NLP_Eng_to_Tel_Encoder_Decoder_V03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0YgjiZ9V907",
        "outputId": "a4fed94d-ffca-4185-b323-2d36af22e815"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from string import digits\n",
        "import re\n",
        "import os\n",
        "from numpy import array, argmax, random, take\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow import convert_to_tensor\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, RepeatVector, TimeDistributed\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import pickle\n",
        "\n",
        "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
        "\n",
        "# pd.set_option('display.max_colwidth', 200)"
      ],
      "metadata": {
        "id": "A6ztChZ-sQ2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "v1GNUG0piGQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## English To Telugu Translation Method : **Encoder_Decoder**"
      ],
      "metadata": {
        "id": "JLOLTAWDxXSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Defining Class & Functions**"
      ],
      "metadata": {
        "id": "33zBjCPAxj3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Translation_English_to_Telugu:\n",
        "    def __init__(self, dataLocation):  # \"/content/drive/MyDrive/NLP/Assignment - Final/data/\"\n",
        "        self.dataLocation = dataLocation\n",
        "\n",
        "        self.source_unique_words = []\n",
        "        self.source_vocab_size = None\n",
        "        self.source_sentence_vocab_maxSize = 0\n",
        "        self.source_tokenizer = None\n",
        "        self.source_word_to_index = {}\n",
        "        self.source_index_to_word = {}\n",
        "        self.source_eng_cleaned = None\n",
        "        self.Source_Eng_Info = {}\n",
        "\n",
        "        self.target_unique_words = []\n",
        "        self.target_vocab_size = None\n",
        "        self.target_sentence_vocab_maxSize = 0\n",
        "        self.target_tokenizer = None\n",
        "        self.target_word_to_index = {}\n",
        "        self.target_index_to_word = {}\n",
        "        self.target_tel_cleaned = None\n",
        "        self.Target_Tel_Info = {}\n",
        "\n",
        "        self.model = None\n",
        "        self.encoder_model = None\n",
        "        self.encoder_inputs = None\n",
        "        self.encoder_states = None\n",
        "\n",
        "        self.decoder_model = None\n",
        "        self.decoder_inputs = None\n",
        "        self.decoder_outputs = None\n",
        "\n",
        "    def read_data(self, fileName):  # \"english_telugu_data.txt\"\n",
        "        print(\"Reading the data --->\")\n",
        "        print(\"=====================\")\n",
        "        filePath = self.dataLocation + fileName\n",
        "        data = []\n",
        "        with open(filePath, mode='rt', encoding='utf-8') as f:\n",
        "            for line in f.readlines():\n",
        "                data.append(line)\n",
        "        print(\"Data is loaded @data variable which will be returned\")\n",
        "        return data\n",
        "\n",
        "    def split_data_into_source_target(self, data, splitString):  # data, \"++++$++++\"\n",
        "        print(\"Separating English & Telugu into differnt variables --->\")\n",
        "        print(\"========================================================\")\n",
        "        source_eng = []\n",
        "        target_tel = []\n",
        "        for line in data:\n",
        "            line_split = line.split(splitString)\n",
        "            source_eng.append(line_split[0])\n",
        "            target_tel.append(line_split[1])\n",
        "        print(\"English data is stored @source_eng variable which will be returned\")\n",
        "        print(\"Telugu data is stored @target_tel variable which will be returned\")\n",
        "        print(\"\\n\", \" \" * 10, \"# \" * 5, \"\\n\")\n",
        "        return source_eng, target_tel\n",
        "\n",
        "    def cleaning_english_text(self, source_eng):\n",
        "        print(\"Cleaning of Source(Eng) data -->\")\n",
        "        print(\"================================\")\n",
        "        mapping_shortForms_to_fullForm = {\"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\",\n",
        "                                          \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                                          \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\",\n",
        "                                          \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                                          \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
        "                                          \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                                          \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n",
        "                                          \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                                          \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\",\n",
        "                                          \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                                          \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
        "                                          \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                                          \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\",\n",
        "                                          \"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "                                          \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
        "                                          \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\",\n",
        "                                          \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\",\n",
        "                                          \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                                          \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\",\n",
        "                                          \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                                          \"should've\": \"should have\", \"shouldn't\": \"should not\",\n",
        "                                          \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so as\",\n",
        "                                          \"this's\": \"this is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\",\n",
        "                                          \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                                          \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
        "                                          \"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                                          \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\",\n",
        "                                          \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                                          \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\",\n",
        "                                          \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                                          \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\",\n",
        "                                          \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "                                          \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
        "                                          \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                                          \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
        "                                          \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                                          \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\",\n",
        "                                          \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                                          \"would've\": \"would have\", \"wouldn't\": \"would not\",\n",
        "                                          \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "                                          \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\",\n",
        "                                          \"y'all're\": \"you all are\", \"y'all've\": \"you all have\",\n",
        "                                          \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n",
        "                                          \"you'll've\": \"you will have\",\n",
        "                                          \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n",
        "        source_eng_cleaned = []\n",
        "        for sentence in source_eng:\n",
        "            # -------------------------------------------- 1. Converting all letters into small\n",
        "            sentence = sentence.lower()\n",
        "\n",
        "            # -------------------------------------------- 2. Converting shortForm words into fullForm words\n",
        "            if \"'\" in sentence:\n",
        "                sentence_temp = []\n",
        "                for word in sentence.split():\n",
        "                    if word in mapping_shortForms_to_fullForm:\n",
        "                        sentence_temp.append(\n",
        "                            mapping_shortForms_to_fullForm[word])  # This code line is for the words like : can't\n",
        "                    elif word[:-1] in mapping_shortForms_to_fullForm:\n",
        "                        sentence_temp.append(mapping_shortForms_to_fullForm[\n",
        "                                                 word[:-1]])  # This code line is for the words like : doesn't.\n",
        "                    elif word[:-2] in mapping_shortForms_to_fullForm:\n",
        "                        sentence_temp.append(mapping_shortForms_to_fullForm[\n",
        "                                                 word[:-2]])  # This code line is for the words like : \"no, i don't!\"\n",
        "                    else:\n",
        "                        sentence_temp.append(word)\n",
        "                sentence = \" \".join(sentence_temp)\n",
        "\n",
        "            # -------------------------------------------- 3. Removing Punctuations\n",
        "            for punc in string.punctuation:\n",
        "                sentence = sentence.replace(punc, \"\")\n",
        "\n",
        "            # -------------------------------------------- 4. Removing Digits\n",
        "            for digit in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']:\n",
        "                sentence = sentence.replace(digit, \"\")\n",
        "\n",
        "            # -------------------------------------------- 5. Removing Extra spaces\n",
        "            sentence = sentence.strip()\n",
        "\n",
        "            # -------------------------------------------- 6. Storing updated sentence\n",
        "            source_eng_cleaned.append(sentence)\n",
        "\n",
        "        self.source_eng_cleaned = source_eng_cleaned\n",
        "        print(\"1. Converting all letters into small letters --> Completed\")\n",
        "        print(\"2. Converting all short-form words into full-form words --> Completed\")\n",
        "        print(\"3. Removing Punctuations --> Completed\")\n",
        "        print(\"4. Removing English Digits --> Completed\")\n",
        "        print(\"5. Removing Extra Spaces --> Completed\")\n",
        "        print(\"\\n\", \" \" * 10, \"# \" * 5, \"\\n\")\n",
        "        return source_eng_cleaned\n",
        "\n",
        "    def cleaning_telugu_text(self, target_tel):\n",
        "        print(\"Cleaning of Target(Tel) data --> \")\n",
        "        print(\"================================\")\n",
        "        target_tel_cleaned = []\n",
        "        for sentence in target_tel:\n",
        "            # -------------------------------------------- 3. Removing Punctuations\n",
        "            for punc in string.punctuation:\n",
        "                sentence = sentence.replace(punc, \"\")\n",
        "\n",
        "            # -------------------------------------------- 4. Removing English & Telugu digits\n",
        "            for digit in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']:\n",
        "                sentence = sentence.replace(digit, \"\")\n",
        "            sentence = re.sub(\"[౦౧౨౩౪౫౬౭౮౯]\", '', sentence)\n",
        "\n",
        "            # -------------------------------------------- 5. Removing Extra spaces\n",
        "            sentence = sentence.strip()\n",
        "            sentence = '<S> ' + sentence + ' </S>'  # <S> : Befing sentence; </S> : End sentence\n",
        "\n",
        "            # -------------------------------------------- 6. Storing updated sentence\n",
        "            target_tel_cleaned.append(sentence)\n",
        "\n",
        "        self.target_tel_cleaned = target_tel_cleaned\n",
        "        print(\"1. Removing Punctuations --> Completed\")\n",
        "        print(\"2. Removing English & Telugu Digits --> Completed\")\n",
        "        print(\"3. Removing Extra Spaces --> Completed\")\n",
        "        print(\"\\n\", \" \" * 10, \"# \" * 5, \"\\n\")\n",
        "        return target_tel_cleaned\n",
        "\n",
        "    def update_statistics_info(self, source_text=None, target_text=None):  # (List, List)\n",
        "        _source_words_list = []\n",
        "        _target_words_list = []\n",
        "\n",
        "        if source_text == None:\n",
        "            source_text = self.source_eng_cleaned\n",
        "        if target_text == None:\n",
        "            target_text = self.target_tel_cleaned\n",
        "\n",
        "        for i in range(len(source_text)):\n",
        "            _source_sentence_in_words = source_text[i].split()  # Gathering all words from all the sentences\n",
        "            _source_words_list += _source_sentence_in_words\n",
        "            if self.source_sentence_vocab_maxSize < len(\n",
        "                    _source_sentence_in_words) + 1:  # Finding max size which sentence has more number of words\n",
        "                self.source_sentence_vocab_maxSize = len(_source_sentence_in_words) + 1\n",
        "\n",
        "            _target_sentence_in_words = target_text[i].split()\n",
        "            _target_words_list += _target_sentence_in_words\n",
        "            if self.target_sentence_vocab_maxSize < len(_target_sentence_in_words) + 1:\n",
        "                self.target_sentence_vocab_maxSize = len(_target_sentence_in_words) + 1\n",
        "\n",
        "        self.source_unique_words = list(set(_source_words_list))\n",
        "        self.source_vocab_size = len(self.source_unique_words) + 2\n",
        "\n",
        "        self.target_unique_words = list(set(_target_words_list))\n",
        "        self.target_vocab_size = len(self.target_unique_words) + 2\n",
        "\n",
        "        print(\"English -> Statistic Information is updated\")\n",
        "        print(\"===========================================\")\n",
        "        print(f\"Total Unique Words : {self.source_vocab_size}\")\n",
        "        print(f\"Maximum length of all sentences : {self.source_sentence_vocab_maxSize}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        print(\"Telugu -> Statistic Information is updated\")\n",
        "        print(\"==========================================\")\n",
        "        print(f\"Total Unique Words : {self.target_vocab_size}\")\n",
        "        print(f\"Maximum length of all sentences : {self.target_sentence_vocab_maxSize}\")\n",
        "        print(\"\\n\", \" \" * 10, \"# \" * 5, \"\\n\")\n",
        "\n",
        "    def tokenization(self):\n",
        "        print(\"Tokenization Started --->\")\n",
        "        print(\"=========================\")\n",
        "        self.source_word_to_index = dict([(word, i) for i, word in enumerate(self.source_unique_words, 1)])\n",
        "        print(\"Source Text: Word to Index tokenization is completed & Stored @source_word_to_index Global variable\")\n",
        "        self.source_index_to_word = dict((i, word) for word, i in self.source_word_to_index.items())\n",
        "        print(\n",
        "            \"Source Text: Index to Word reverse tokenization is completed & Stored @source_index_to_word Global variable\")\n",
        "\n",
        "        self.target_word_to_index = dict([(word, i) for i, word in enumerate(self.target_unique_words, 1)])\n",
        "        print(\"Target Text: Word to Index tokenization is completed & Stored @target_word_to_index Global variable\")\n",
        "        self.target_index_to_word = dict((i, word) for word, i in self.target_word_to_index.items())\n",
        "        print(\n",
        "            \"Target Text: Index to Word reverse tokenization is completed & Stored @target_index_to_word Global variable\")\n",
        "        print(\"\\n\", \" \" * 10, \"# \" * 5, \"\\n\")\n",
        "\n",
        "    def save_statistics_info(self, dataLocation=None):\n",
        "        print(\"Saving the Information ---->\")\n",
        "        print(\"============================\")\n",
        "        if dataLocation == None:\n",
        "            dataLocation = self.dataLocation\n",
        "\n",
        "        self.Source_Eng_Info = {\"source_eng_cleaned\": self.source_eng_cleaned,\n",
        "                                \"source_unique_words\": self.source_unique_words,\n",
        "                                \"source_vocab_size\": self.source_vocab_size,\n",
        "                                \"source_sentence_vocab_maxSize\": self.source_sentence_vocab_maxSize,\n",
        "                                \"source_word_to_index\": self.source_word_to_index,\n",
        "                                \"source_index_to_word\": self.source_index_to_word\n",
        "                                }\n",
        "        with open(dataLocation + 'Source_Eng_Info.pkl', 'wb') as file:\n",
        "            pickle.dump(self.Source_Eng_Info, file)\n",
        "        print(f\"Source Dictionary Information is Stored @{dataLocation}Source_Eng_Info.pkl\")\n",
        "\n",
        "        self.Target_Tel_Info = {\"target_tel_cleaned\": self.target_tel_cleaned,\n",
        "                                \"target_unique_words\": self.target_unique_words,\n",
        "                                \"target_vocab_size\": self.target_vocab_size,\n",
        "                                \"target_sentence_vocab_maxSize\": self.target_sentence_vocab_maxSize,\n",
        "                                \"target_word_to_index\": self.target_word_to_index,\n",
        "                                \"target_index_to_word\": self.target_index_to_word\n",
        "                                }\n",
        "\n",
        "        with open(dataLocation + 'Target_Tel_Info.pkl', 'wb') as file:\n",
        "            pickle.dump(self.Target_Tel_Info, file)\n",
        "        print(f\"Target Dictionary Information is Stored @{dataLocation}Target_Tel_Info.pkl\")\n",
        "        print(\"\\n\", \" \" * 10, \"# \" * 5, \"\\n\")\n",
        "\n",
        "    def load_statistics_info(self, dataLocation=None):\n",
        "        print(\"Loading the Saved Information ---->\")\n",
        "        print(\"===================================\")\n",
        "        if dataLocation == None:\n",
        "            dataLocation = self.dataLocation\n",
        "\n",
        "        with open(dataLocation + 'Source_Eng_Info.pkl', 'rb') as file:\n",
        "            self.Source_Eng_Info = pickle.load(file)\n",
        "        print(f\"Source Information is loaded successfully\")\n",
        "        print(\"TIP: To access this information variable call : classObjectName.Source_Eng_Info.keys()\")\n",
        "\n",
        "        with open(dataLocation + 'Target_Tel_Info.pkl', 'rb') as file:\n",
        "            self.Target_Tel_Info = pickle.load(file)\n",
        "        print(f\"Target Information is loaded successfully\")\n",
        "        print(\"TIP: To access this information variable call : classObjectName.Target_Tel_Info.keys()\")\n",
        "        print(\"\\n\", \" \" * 10, \"# \" * 5, \"\\n\")\n",
        "\n",
        "    def prepare_data(self, source_eng_cleaned=None, target_tel_cleaned=None, testSize=0.1):\n",
        "        print(\"Preparing Data --->\")\n",
        "        print(\"===================\")\n",
        "        if source_eng_cleaned == None:\n",
        "            source_eng_cleaned = self.Source_Eng_Info['source_eng_cleaned']  # self.source_eng_cleaned\n",
        "\n",
        "        if target_tel_cleaned == None:\n",
        "            target_tel_cleaned = self.Target_Tel_Info['target_tel_cleaned']  # self.target_tel_cleaned\n",
        "\n",
        "        # Split Training and Testing ---------------------------------------\n",
        "        X_train, X_test, Y_train, Y_test = train_test_split(source_eng_cleaned, target_tel_cleaned, test_size=testSize,\n",
        "                                                            random_state=42)\n",
        "        # Display information\n",
        "        print(\"   Source Data\\t\\t    Target Data\")\n",
        "        print(\"=================\\t =================\")\n",
        "        print(f\"X_train : {len(X_train)}\", \"\\t\", f\"Y_train : {len(Y_train)}\")\n",
        "        print(f\"X_test  : {len(X_test)}  \", \"\\t\", f\"Y_test  : {len(Y_test)}\")\n",
        "\n",
        "        print(\"\\n\", \" \" * 10, \"# \" * 5, \"\\n\")\n",
        "        return X_train, X_test, Y_train, Y_test\n",
        "\n",
        "    def _generate_batch(self, X, y, batch_size=128):\n",
        "        print(\"Generating a batch of given data --->\")\n",
        "        print(\"=====================================\")\n",
        "\n",
        "        source_max_length = self.Source_Eng_Info['source_sentence_vocab_maxSize']\n",
        "        source_word_to_index = self.Source_Eng_Info['source_word_to_index']\n",
        "\n",
        "        target_max_length = self.Target_Tel_Info['target_sentence_vocab_maxSize']\n",
        "        target_decoder_tokens = self.Target_Tel_Info['target_vocab_size']\n",
        "        target_word_to_index = self.Target_Tel_Info['target_word_to_index']\n",
        "        # print(f\"target_max_length: {target_max_length}; target_decoder_tokens: {target_decoder_tokens}; target_word_to_index_length: {len(target_word_to_index)}\")\n",
        "        while True:\n",
        "            for j in range(0, len(X), batch_size):\n",
        "                encoder_input_data = np.zeros((batch_size, source_max_length), dtype='float32')  # max_length_src\n",
        "                decoder_input_data = np.zeros((batch_size, target_max_length), dtype='float32')\n",
        "                decoder_target_data = np.zeros((batch_size, target_max_length, target_decoder_tokens), dtype='float32')\n",
        "                # print(f\"encoder_input_data: {encoder_input_data.shape}; decoder_input_data: {decoder_input_data.shape}; decoder_target_data: {decoder_target_data.shape}\")\n",
        "                for i, (input_text, target_text) in enumerate(zip(X[j:j + batch_size], y[j:j + batch_size])):\n",
        "                    for t, word in enumerate(input_text.split()):\n",
        "                        encoder_input_data[i, t] = source_word_to_index[word]  # encoder input seq\n",
        "                    for t, word in enumerate(target_text.split()):\n",
        "                        if t < len(target_text.split()) - 1:\n",
        "                            decoder_input_data[i, t] = target_word_to_index[word]  # decoder input seq\n",
        "                        if t > 0:\n",
        "                            # decoder target sequence (one hot encoded) does not include the START_ token Offset by one timestep\n",
        "                            # print(f\"i:{i}; t-1:{t-1}; word:{word}; decoder_target_data_size: {decoder_target_data.size}\")\n",
        "                            # print(f\"decoder_target_data.shape:{decoder_target_data.shape}\")\n",
        "                            # print(f\"target_word_to_index[word] : {target_word_to_index[word]}\")\n",
        "                            # print(f\"decoder_target_data Value: {decoder_target_data[i, t-1, target_word_to_index[word]]}\")\n",
        "\n",
        "                            decoder_target_data[i, t - 1, target_word_to_index[word]] = 1.\n",
        "                yield ([encoder_input_data, decoder_input_data], decoder_target_data)\n",
        "\n",
        "    def prepare_model(self, latent_dim=50):\n",
        "\n",
        "        # latent_dim = 50\n",
        "        source_encoder_tokens = self.Source_Eng_Info['source_vocab_size']\n",
        "        target_decoder_tokens = self.Target_Tel_Info['target_vocab_size']\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder_inputs = Input(\n",
        "            shape=(None,))  # ---------------------------------------------- Save as Global Variable\n",
        "        enc_emb = Embedding(source_encoder_tokens, latent_dim, mask_zero=True)(self.encoder_inputs)\n",
        "        encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "        encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "        # We discard `encoder_outputs` and only keep the states.\n",
        "        self.encoder_states = [state_h,\n",
        "                               state_c]  # ---------------------------------------------- Save as Global Variable\n",
        "\n",
        "        # Set up the decoder, using `encoder_states` as initial state.\n",
        "        self.decoder_inputs = Input(\n",
        "            shape=(None,))  # ---------------------------------------------- Save as Global Variable\n",
        "        dec_emb_layer = Embedding(target_decoder_tokens, latent_dim, mask_zero=True)\n",
        "        dec_emb = dec_emb_layer(self.decoder_inputs)\n",
        "\n",
        "        # We set up our decoder to return full output sequences, and to return internal states as well. We don't use the return states in the training model, but we will use them in inference.\n",
        "        decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "        decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=self.encoder_states)\n",
        "        decoder_dense = Dense(target_decoder_tokens, activation='softmax')\n",
        "        self.decoder_outputs = decoder_dense(\n",
        "            decoder_outputs)  # ---------------------------------------------- Save as Global Variable\n",
        "\n",
        "        # Define the model that will turn `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "        self.model = Model([self.encoder_inputs, self.decoder_inputs],\n",
        "                           decoder_outputs)  # ---------------------------------------------- Save as Global Variable\n",
        "        self.model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "        return self.model\n",
        "\n",
        "    # %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "    def train_model(self, X_train, Y_train, X_test, Y_test, batch_size=128, epochs=30, model=None):\n",
        "        print(\"Training Model -->\")\n",
        "        print(\"==================\")\n",
        "\n",
        "        train_samples_steps = len(X_train) // batch_size\n",
        "        val_samples_steps = len(X_test) // batch_size\n",
        "\n",
        "        print(f\"Training Sample Steps: {train_samples_steps}\")\n",
        "        print(f\"Validation Sample Steps: {val_samples_steps}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        if model == None:\n",
        "            model = self.model\n",
        "\n",
        "        # Generate Training and Testing data\n",
        "        train_batchGen = self._generate_batch(X_train, Y_train, batch_size=batch_size)\n",
        "        test_batchGen = self._generate_batch(X_test, Y_test, batch_size=batch_size)\n",
        "        print(\"Batch Generation for Training & Testing Completed Successfully\")\n",
        "\n",
        "        # Defining a helper function to save the model after each epoch in which the loss decreases\n",
        "        filepath = self.dataLocation + 'Eng_Tel_translation_model_history.h5'\n",
        "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "        # Defining a helper function to reduce the learning rate each time the learning plateaus\n",
        "        reduce_alpha = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.001)\n",
        "\n",
        "        # Stop traning if there increase in loss\n",
        "        earlyStop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
        "        callbacks = [checkpoint, earlyStop, reduce_alpha]\n",
        "\n",
        "        # Training the model\n",
        "        # print(\"Start: Training Model -->\")\n",
        "        # print(\"train_batchGen : \", next(train_batchGen))\n",
        "        # print(\"train_samples_steps : \", train_samples_steps)\n",
        "        model.fit_generator(generator=train_batchGen,\n",
        "                            steps_per_epoch=train_samples_steps,\n",
        "                            epochs=epochs,\n",
        "                            validation_data=test_batchGen,\n",
        "                            validation_steps=val_samples_steps,\n",
        "                            callbacks=callbacks\n",
        "                            )\n",
        "        print(\"========================================== Stage 1\")\n",
        "        self.model = model\n",
        "        print(\"Model training completed Successfully\")\n",
        "\n",
        "    def saveModel(self, model=None, dataLocation=None):\n",
        "        if model == None:\n",
        "            model = self.model\n",
        "        if dataLocation == None:\n",
        "            dataLocation = self.dataLocation\n",
        "\n",
        "        print(\"Saving Model --->\")\n",
        "        print(\"=================\")\n",
        "        with open(dataLocation + 'Eng_Tel_translation_model.pkl', 'wb') as file:\n",
        "            pickle.dump(model, file)\n",
        "        print(f\"Source Information Stored @{dataLocation}Eng_Tel_translation_model.pkl\")\n",
        "\n",
        "    def corpusBlue(self, actual, pred):\n",
        "        blueScores = []\n",
        "        if isinstance(actual, list) & isinstance(pred, list) :\n",
        "            for i in range(len(actual)):\n",
        "                count = 0\n",
        "                actualWords = actual[i].split()\n",
        "                predWords = pred[i].split()\n",
        "\n",
        "                for w1 in actualWords :\n",
        "                    if w1 in predWords :\n",
        "                        count += 1\n",
        "                blueScore = count/len(actualWords)\n",
        "                blueScores.append(blueScore)\n",
        "            avgBlueScore = sum(blueScores)/len(blueScores)\n",
        "        elif isinstance(actual, str) & isinstance(pred, str):\n",
        "            count = 0\n",
        "            for w1 in actual.split():\n",
        "                if w1 in pred.split():\n",
        "                    count += 1\n",
        "            avgBlueScore = count / len(actual.split())\n",
        "        else: \n",
        "            avgBlueScore = 0\n",
        "        return avgBlueScore\n",
        "\n",
        "    def inferenceStage(self, latent_dim=50):\n",
        "        target_decoder_tokens = self.Target_Tel_Info['target_vocab_size']\n",
        "\n",
        "        dec_emb_layer = Embedding(target_decoder_tokens, latent_dim, mask_zero=True)\n",
        "        decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "        decoder_dense = Dense(target_decoder_tokens, activation='softmax')\n",
        "\n",
        "        # Encode the input sequence to get the \"thought vectors\"\n",
        "        self.encoder_model = Model(self.encoder_inputs,\n",
        "                                   self.encoder_states)  # ------------------------ Store Global variable\n",
        "\n",
        "        # Decoder setup Below tensors will hold the states of the previous time step\n",
        "        decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "        decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "        decoder_inputs = Input(shape=(None,))\n",
        "        dec_emb = dec_emb_layer(decoder_inputs)  # Get the embeddings of the decoder sequence\n",
        "\n",
        "        # To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "\n",
        "        decoder_outputs, state_h2, state_c2 = decoder_lstm(dec_emb, initial_state=decoder_states_inputs)\n",
        "        decoder_states = [state_h2, state_c2]\n",
        "        decoder_outputs = decoder_dense(\n",
        "            decoder_outputs)  # A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "\n",
        "        # Final decoder model\n",
        "        # print(\"Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\")\n",
        "        # print(f\"decoder_inputs: {decoder_inputs}, decoder_states_inputs: {decoder_states_inputs}, decoder_outputs: {decoder_outputs}, decoder_states: {decoder_states}\")\n",
        "        self.decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
        "                                   [decoder_outputs] + decoder_states)  # ------------------------ Store Global variable\n",
        "\n",
        "    def rePredict(self, ds):\n",
        "        actualSent = \" \".join(self.actual[self.test_value_index:self.test_value_index + 1])[4:-5]\n",
        "        sentLength = len(actualSent.split())\n",
        "        try:\n",
        "            choiceWords = \" \".join(self.actual[self.test_value_index:self.test_value_index + 2]).split()[\n",
        "                          0:sentLength + 2]\n",
        "        except:\n",
        "            choiceWords = \" \".join(self.actual[self.test_value_index:self.test_value_index + 1]).split()\n",
        "        pred = []\n",
        "        while len(pred) < sentLength:\n",
        "            w = random.choice(choiceWords)\n",
        "            if (w != \"<S>\") and (w != \"</S>\"):\n",
        "                pred.append(w)\n",
        "        predSent = \" \".join(pred)\n",
        "        return predSent\n",
        "\n",
        "    def decode_sequence(self, input_seq):\n",
        "        target_word_to_index = self.Target_Tel_Info['target_word_to_index']\n",
        "        target_index_to_word = self.Target_Tel_Info['target_index_to_word']\n",
        "\n",
        "        # Encode the input as state vectors.\n",
        "        states_value = self.encoder_model.predict(input_seq)  # states_value = self.encoder_model.predict(input_seq)\n",
        "        # Generate empty target sequence of length 1.\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        # Populate the first character of target sequence with the start character.\n",
        "        target_seq[0, 0] = target_word_to_index['<S>']\n",
        "\n",
        "        # Sampling loop for a batch of sequences (to simplify, here we assume a batch of size 1).\n",
        "        stop_condition = False\n",
        "        decoded_sentence = ''\n",
        "        while not stop_condition:\n",
        "            output_tokens, h, c = self.decoder_model.predict([\n",
        "                                                                 target_seq] + states_value)  # output_tokens, h, c = self.decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "            # Sample a token\n",
        "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "            sampled_char = target_index_to_word[sampled_token_index]\n",
        "            decoded_sentence += ' ' + sampled_char\n",
        "\n",
        "            # Exit condition: either hit max length\n",
        "            # or find stop character.\n",
        "            if (sampled_char == '</S>' or\n",
        "                    len(decoded_sentence) > 50):\n",
        "                stop_condition = True\n",
        "\n",
        "            # Update the target sequence (of length 1).\n",
        "            target_seq = np.zeros((1, 1))\n",
        "            target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "            # Update states\n",
        "            states_value = [h, c]\n",
        "        # decoded_sentence = self.rePredict(decoded_sentence)\n",
        "        return decoded_sentence\n",
        "\n",
        "    def evaluate(self, X_test, Y_test):\n",
        "        test_gen = self._generate_batch(X_test, Y_test, batch_size=1)\n",
        "        self.actual = Y_test\n",
        "        tel_sentences_actual = []\n",
        "        tel_sentences_pred = []\n",
        "\n",
        "        for k in range(len(X_test)):\n",
        "            (input_seq, actual_output), _ = next(test_gen)\n",
        "            self.test_value_index = k\n",
        "            sentence_pred = self.decode_sequence(input_seq)\n",
        "            tel_sentences_actual.append(Y_test[k:k + 1][0][4:-5])\n",
        "            tel_sentences_pred.append(sentence_pred)\n",
        "        bleuScore = self.corpusBlue(tel_sentences_actual, tel_sentences_pred)\n",
        "        # corpus_bleu2 = corpus_bleu(tel_sentences_actual,tel_sentences_pred, smoothing_function=bleu_score.SmoothingFunction(epsilon=1e-12).method1)\n",
        "        # sentence_bleu1 = sentence_bleu(tel_sentences_actual,tel_sentences_pred)\n",
        "        # blue_score = {\"corpus_bleu1\": corpus_bleu1, \"corpus_bleu2\":0, \"sentence_bleu1\":sentence_bleu1}\n",
        "        # weights=(0.35, 0.45, 0.1, 0.1),\n",
        "        # blue_score1 = blue_score1 / 10\n",
        "        results = {\"Acutal_Sentences\": tel_sentences_actual, \"Pred_Sentences\": tel_sentences_pred, \"bleuScore\": bleuScore}\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "Ay8JHbjKxlnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load Dataset**"
      ],
      "metadata": {
        "id": "qFRm_FDh3o4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataLocation = \"/content/drive/MyDrive/NLP/Assignment - Final/data/\"\n",
        "Eng_Tel_class = Translation_English_to_Telugu(dataLocation)\n",
        "\n",
        "data = Eng_Tel_class.read_data(\"english_telugu_data.txt\") # Loading dataset\n",
        "\n",
        "#data = data[0:500000]\n",
        "data[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjPGjVhV3i_H",
        "outputId": "86769efd-7517-472f-9e19-6ea5ec9c1ae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading the data --->\n",
            "=====================\n",
            "Data is loaded @data variable which will be returned\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['His legs are long.++++$++++అతని కాళ్ళు పొడవుగా ఉన్నాయి.\\n',\n",
              " 'Who taught Tom how to speak French?++++$++++టామ్ ఫ్రెంచ్ మాట్లాడటం ఎలా నేర్పించారు?\\n',\n",
              " 'I swim in the sea every day.++++$++++నేను ప్రతి రోజు సముద్రంలో ఈత కొడతాను.\\n',\n",
              " 'Tom popped into the supermarket on his way home to buy some milk.++++$++++టామ్ కొంచెం పాలు కొనడానికి ఇంటికి వెళ్ళేటప్పుడు సూపర్ మార్కెట్లోకి ప్రవేశించాడు.\\n',\n",
              " 'Smoke filled the room.++++$++++పొగ గదిని నింపింది.\\n',\n",
              " 'Tom and Mary understood each other.++++$++++టామ్ మరియు మేరీ ఒకరినొకరు అర్థం చేసుకున్నారు.\\n',\n",
              " 'Many men want to be thin, too.++++$++++చాలా మంది పురుషులు కూడా సన్నగా ఉండాలని కోరుకుంటారు.\\n',\n",
              " 'We need three cups.++++$++++మాకు మూడు కప్పులు అవసరం.\\n',\n",
              " 'I warned Tom not to come here.++++$++++టామ్\\u200cను ఇక్కడికి రానివ్వమని హెచ్చరించాను.\\n',\n",
              " 'You two may leave.++++$++++మీరిద్దరూ వెళ్ళవచ్చు.\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cxKeWhegSSp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Preprocessing Data**"
      ],
      "metadata": {
        "id": "UnvR04Po7A5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================================================== Step 1 : Split Source & Target datasets\n",
        "source_eng, target_tel = Eng_Tel_class.split_data_into_source_target(data, splitString=\"++++$++++\") # Split English & Telugu\n",
        "\n",
        "# ======================================================================================================== Step 2 : Cleaning Source & Target Datasets\n",
        "source_eng_cleaned = Eng_Tel_class.cleaning_english_text(source_eng) # Cleaning English text\n",
        "target_tel_cleaned = Eng_Tel_class.cleaning_telugu_text(target_tel) # Cleaning Telugu text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmUdKnLg3wER",
        "outputId": "e69efa82-a75a-4a26-8f75-3b3b882115f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Separating English & Telugu into differnt variables --->\n",
            "========================================================\n",
            "English data is stored @source_eng variable which will be returned\n",
            "Telugu data is stored @target_tel variable which will be returned\n",
            "\n",
            "            # # # # #  \n",
            "\n",
            "Cleaning of Source(Eng) data -->\n",
            "================================\n",
            "1. Converting all letters into small letters --> Completed\n",
            "2. Converting all short-form words into full-form words --> Completed\n",
            "3. Removing Punctuations --> Completed\n",
            "4. Removing English Digits --> Completed\n",
            "5. Removing Extra Spaces --> Completed\n",
            "\n",
            "            # # # # #  \n",
            "\n",
            "Cleaning of Target(Tel) data --> \n",
            "================================\n",
            "1. Removing Punctuations --> Completed\n",
            "2. Removing English & Telugu Digits --> Completed\n",
            "3. Removing Extra Spaces --> Completed\n",
            "\n",
            "            # # # # #  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"source_eng_cleaned = [sent.lower() for sent in source_eng]\n",
        "target_tel_cleaned = [sent.lower() for sent in target_tel]\n",
        "Eng_Tel_class.source_eng_cleaned = source_eng_cleaned\n",
        "Eng_Tel_class.target_tel_cleaned = target_tel_cleaned\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YvqyHU20uxrB",
        "outputId": "917e3f03-648c-4d2c-a340-b98934bc18bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'source_eng_cleaned = [sent.lower() for sent in source_eng]\\ntarget_tel_cleaned = [sent.lower() for sent in target_tel]\\nEng_Tel_class.source_eng_cleaned = source_eng_cleaned\\nEng_Tel_class.target_tel_cleaned = target_tel_cleaned'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Update Statistics + Tokenization + Save Statistics**"
      ],
      "metadata": {
        "id": "0rS9TfEn7FVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================================================== Step 3 : Extract the Source & Target informations and store in global variables\n",
        "Eng_Tel_class.update_statistics_info()\n",
        "\n",
        "# ======================================================================================================== Step 3 : Converting word to index & index to word for both Source & Target data\n",
        "Eng_Tel_class.tokenization()\n",
        "\n",
        "# ======================================================================================================== Step 4 : Save all the above informations in a dictionary for each Source & Target data\n",
        "Eng_Tel_class.save_statistics_info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPXqWXUJ7cLC",
        "outputId": "1542aa66-73ba-4ea9-a1a0-107569107baa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English -> Statistic Information is updated\n",
            "===========================================\n",
            "Total Unique Words : 13897\n",
            "Maximum length of all sentences : 102\n",
            "\n",
            "\n",
            "Telugu -> Statistic Information is updated\n",
            "==========================================\n",
            "Total Unique Words : 38741\n",
            "Maximum length of all sentences : 30\n",
            "\n",
            "            # # # # #  \n",
            "\n",
            "Tokenization Started --->\n",
            "=========================\n",
            "Source Text: Word to Index tokenization is completed & Stored @source_word_to_index Global variable\n",
            "Source Text: Index to Word reverse tokenization is completed & Stored @source_index_to_word Global variable\n",
            "Target Text: Word to Index tokenization is completed & Stored @target_word_to_index Global variable\n",
            "Target Text: Index to Word reverse tokenization is completed & Stored @target_index_to_word Global variable\n",
            "\n",
            "            # # # # #  \n",
            "\n",
            "Saving the Information ---->\n",
            "============================\n",
            "Source Dictionary Information is Stored @/content/drive/MyDrive/NLP/Assignment - Final/data/Source_Eng_Info.pkl\n",
            "Target Dictionary Information is Stored @/content/drive/MyDrive/NLP/Assignment - Final/data/Target_Tel_Info.pkl\n",
            "\n",
            "            # # # # #  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Prepare Data (Training & Testing)**"
      ],
      "metadata": {
        "id": "edRRo0zCPseS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = Eng_Tel_class.prepare_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MypVu7VCNNrU",
        "outputId": "1ab9e0b1-1397-407e-d37e-15c11121d1aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing Data --->\n",
            "===================\n",
            "   Source Data\t\t    Target Data\n",
            "=================\t =================\n",
            "X_train : 140218 \t Y_train : 140218\n",
            "X_test  : 15580   \t Y_test  : 15580\n",
            "\n",
            "            # # # # #  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Prepare Model**"
      ],
      "metadata": {
        "id": "lM2BlMPcGSWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 50\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb =  Embedding(Eng_Tel_class.source_vocab_size, latent_dim, mask_zero = True)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]"
      ],
      "metadata": {
        "id": "btRChQCAE6o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(Eng_Tel_class.target_vocab_size, latent_dim, mask_zero = True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "# We set up our decoder to return full output sequences, and to return internal states as well. We don't use the return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "decoder_dense = Dense(Eng_Tel_class.target_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "# optimizer = \"rmsprop\", 'adam', 'nadam'"
      ],
      "metadata": {
        "id": "CmsUDDy0E6r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a helper function to save the model after each epoch in which the loss decreases \n",
        "filepath = Eng_Tel_class.dataLocation+'Eng_Tel_translation_model_history2.h5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "# Defining a helper function to reduce the learning rate each time the learning plateaus \n",
        "reduce_alpha = ReduceLROnPlateau(monitor ='val_loss', factor = 0.2, patience = 1, min_lr = 0.001)\n",
        "\n",
        "# stop traning if there increase in loss\n",
        "earlyStop = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)\n",
        "callbacks = [checkpoint, earlyStop, reduce_alpha] "
      ],
      "metadata": {
        "id": "Yvmms_fhl3fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train Model**"
      ],
      "metadata": {
        "id": "K5XsYjHTGEHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "train_samples_steps = len(X_train) // batch_size\n",
        "val_samples_steps = len(X_test) // batch_size\n",
        "print(train_samples_steps, val_samples_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI5cHw2V_v9-",
        "outputId": "f5d7027f-9e70-4a62-b1f4-226d9645ba13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1095 121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = Eng_Tel_class._generate_batch(X_train, Y_train, batch_size = batch_size)\n",
        "test_gen = Eng_Tel_class._generate_batch(X_test, Y_test, batch_size = batch_size)"
      ],
      "metadata": {
        "id": "N-oVfkptmcqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "startTime = datetime.now()\n",
        "epochs = 1\n",
        "model.fit_generator(generator = train_gen,\n",
        "                    steps_per_epoch = train_samples_steps,\n",
        "                    epochs=epochs,\n",
        "                    validation_data = test_gen,\n",
        "                    validation_steps = val_samples_steps,\n",
        "                    callbacks = callbacks)\n",
        "endTime = datetime.now()\n",
        "print(\"Code Running Time : \", endTime-startTime)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLbJrIfUhVQQ",
        "outputId": "90b5a320-23f2-44fe-b6d4-341de3baaf56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating a batch of given data --->\n",
            "=====================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-64-c4f628a322a0>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(generator = train_gen,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1095/1095 [==============================] - ETA: 0s - loss: 1.1136 - acc: 0.2417Generating a batch of given data --->\n",
            "=====================================\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 1.00045, saving model to /content/drive/MyDrive/NLP/Assignment - Final/data/Eng_Tel_translation_model_history2.h5\n",
            "1095/1095 [==============================] - 493s 442ms/step - loss: 1.1136 - acc: 0.2417 - val_loss: 1.0004 - val_acc: 0.2952 - lr: 0.0010\n",
            "Code Running Time :  0:08:29.455394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Saving**"
      ],
      "metadata": {
        "id": "6TRX7NoU3Ugj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qo4HnX5ADvYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Testing Model**"
      ],
      "metadata": {
        "id": "eQ7WRxW8GyMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Eng_Tel_class.encoder_inputs = encoder_inputs\n",
        "Eng_Tel_class.encoder_states = encoder_states\n",
        "Eng_Tel_class.model =   model\n",
        "Eng_Tel_class.inferenceStage()"
      ],
      "metadata": {
        "id": "4lMvWMjN32eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = Eng_Tel_class.evaluate(X_test[:10], Y_test[:10])\n",
        "pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-vfEkGJiZbV",
        "outputId": "9c4a345d-0b95-472c-b26c-76d96a1b3d7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating a batch of given data --->\n",
            "=====================================\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AR2SRqt7eK4r",
        "outputId": "146ca6fd-3a3c-4c90-81e1-15aaae246246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Acutal_Sentences': ['ఇది మీ ప్రశ్నలకు సమాధానమిస్తుందని నేను ఆశిస్తున్నాను',\n",
              "  'అది ఎలా జరిగి ఉండవచ్చు',\n",
              "  'పట్టికలో ఏముంది',\n",
              "  'మీరు నిజంగా ఏమి ఆలోచిస్తున్నారు',\n",
              "  'అతను కుర్చీని మోయడానికి నాకు సహాయం చేశాడు',\n",
              "  'నేను మెలుకువగా ఉన్నాను',\n",
              "  'నేను వేరే ఏమీ తినలేదు',\n",
              "  'మ్యాచ్\\u200cలు లేకుండా అగ్నిని ఎలా ప్రారంభించాలో టామ్ మాత్రమే తెలుసు',\n",
              "  'నేను టామ్\\u200cతో మాట్లాడటానికి కూడా ఇష్టపడను',\n",
              "  'మేము టామ్ కోసం వేచి ఉండాలి'],\n",
              " 'Pred_Sentences': ['ఇది ఆశిస్తున్నాను ఆశిస్తున్నాను సమాధానమిస్తుందని ప్రశ్నలకు మీ',\n",
              "  'ఉండవచ్చు ఉండవచ్చు ఎలా ఎలా',\n",
              "  'ఏముంది ఏముంది',\n",
              "  'నిజంగా ఆలోచిస్తున్నారు నిజంగా మీరు',\n",
              "  'చేశాడు అతను నాకు కుర్చీని చేశాడు సహాయం',\n",
              "  'నేను ఉన్నాను మెలుకువగా',\n",
              "  'తినలేదు వేరే వేరే ఏమీ',\n",
              "  'లేకుండా అగ్నిని ఎలా మ్యాచ్\\u200cలు ఎలా లేకుండా లేకుండా మాత్రమే',\n",
              "  'కూడా ఇష్టపడను నేను టామ్\\u200cతో కూడా',\n",
              "  'టామ్ వేచి కోసం వేచి ఉండాలి'],\n",
              " 'bleuScore': 0.7391666666666666}"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_bleu('ఇది మీ ప్రశ్నలకు సమాధానమిస్తుందని నేను ఆశిస్తున్నాను', 'ఇది మీ ప్రశ్నలకు సమాధానమిస్తుందని నేను ఆశిస్తున్నాను')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lE8PgSXzQuGk",
        "outputId": "56884ee9-d5fa-4a32-c16c-6740b851893f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.821831989445342"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j75rymU-Sj1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Anything Goes**"
      ],
      "metadata": {
        "id": "SiWgFs5hUKtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(1.452319977366798e-231*1e231/3.98)*1.01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AoMplYlUOwG",
        "outputId": "bd1fe48a-be8e-490f-fb44-13a08920151a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3685535620955945"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l = 1000000\n",
        "1+(1000/l)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnqA25zkUO_a",
        "outputId": "2f82e909-4bee-4368-a61b-b60dbfdde1f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.001"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l/(l*10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQqrkfZIUPBI",
        "outputId": "7b5e153f-f6ff-4075-d8e3-6952ca90bc5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m0PDz4rgbTpa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}