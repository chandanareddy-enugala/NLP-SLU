{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WBehfvIb5ddr",
        "kWcmVxYLnG9_",
        "02CNlfd1nNv9",
        "G3BrDjkMpCeh",
        "NT-PfebipIYf",
        "OoKoNpQapPoq",
        "Lb7IVRWwqQWX"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chandanareddy-enugala/NLP-SLU/blob/main/NLP_HMM_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDS6ekfBVIqj",
        "outputId": "9120ae12-5d66-4acc-ff69-4c0e39732aca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Libraries & Loading Dataset"
      ],
      "metadata": {
        "id": "WBehfvIb5ddr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pprint, time\n",
        "import random"
      ],
      "metadata": {
        "id": "LldTRZDjm8wS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load Dataset**"
      ],
      "metadata": {
        "id": "kWcmVxYLnG9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filePath = \"/content/drive/MyDrive/NLP/data/train.tsv\"\n",
        "data = pd.read_csv(filePath, lineterminator='\\n',error_bad_lines=False, sep = '\\t',header= None)\n",
        "data = data[data[0].apply(lambda x: len(str(x))<18)]\n",
        "# data[2] = data[0].apply(lambda x: len(str(x)))\n",
        "# data = data[data[2]<18]\n",
        "data = data.reset_index(drop=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lp_XvMgwm9ks",
        "outputId": "aaf71580-eaaf-4af2-e2c0-4dc7437d5418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:3326: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split the dataset into Training and Testing"
      ],
      "metadata": {
        "id": "02CNlfd1nNv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split data into training and validation set in the ratio 80:20\n",
        "train_set, test_set = train_test_split(data, train_size=0.80, test_size=0.20, random_state = 101)\n",
        "splitIndex = list(data[data[0]==\"<S>\"].index)[-100]\n",
        "train_set = data.loc[0:5041631]\n",
        "test_set = data.loc[5041631:]"
      ],
      "metadata": {
        "id": "A-jW404_nMJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding Tags, Words, Tagscount"
      ],
      "metadata": {
        "id": "G3BrDjkMpCeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_tagsCount(data):\n",
        "  only_words_data = data[data[0]!='<S>'].reset_index(drop=True)\n",
        "  tag_value_counts = only_words_data[1].value_counts()\n",
        "  tags_count = {}\n",
        "  for key in tag_value_counts.keys():\n",
        "    tags_count[key] = tag_value_counts[key]\n",
        "  \n",
        "  tags = list(tags_count.keys())\n",
        "  words = list(only_words_data[0])\n",
        "\n",
        "  return tags_count, tags, words"
      ],
      "metadata": {
        "id": "VxWDpynsnMMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags_count, tags, words = find_tagsCount(train_set)\n",
        "tags_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o04L_vVhnMPv",
        "outputId": "42461df0-5543-4ec5-afbb-9019439bb295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'N': 4125188, 'S': 492555, 'U': 165642, 'H': 40378, 'T': 17707}"
            ]
          },
          "metadata": {},
          "execution_count": 325
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding Words Emission Counts and Probability Tables"
      ],
      "metadata": {
        "id": "NT-PfebipIYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_words_emission_CountProbTables(data):\n",
        "  only_words_data = data[data[0]!='<S>'].reset_index(drop=True)\n",
        "  words_emission_count = pd.crosstab(only_words_data[0], only_words_data[1])\n",
        "  words_emission_prob = words_emission_count.copy()\n",
        "  for tag in tags:\n",
        "    words_emission_prob[tag] /= words_emission_prob[tag].sum()\n",
        "  return words_emission_count, words_emission_prob"
      ],
      "metadata": {
        "id": "4o1l0-k2n2-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_emission_count, words_emission_prob = find_words_emission_CountProbTables(train_set)"
      ],
      "metadata": {
        "id": "WqQ8grLuoNuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding Tags Transition Counts and Probabilities Table"
      ],
      "metadata": {
        "id": "OoKoNpQapPoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_tags_transition_CountProbTables(data):\n",
        "  data_S = data[data[0]==\"<S>\"]\n",
        "  data_S.columns = [1,0]\n",
        "  data_not_S = data[data[0]!=\"<S>\"]\n",
        "  data_copy = pd.concat([data_not_S, data_S])\n",
        "  data_copy = data_copy.sort_index()\n",
        "\n",
        "  data_copy[2] = data_copy[1].shift(1)\n",
        "  data_copy.loc[0, 2] = \"<S>\"\n",
        "  data_copy\n",
        "\n",
        "  tags_transition_count = pd.crosstab(data_copy[2], data_copy[1])\n",
        "  tags_transition_count.loc['<E>', :] = tags_transition_count['<S>']\n",
        "  tags_transition_count = tags_transition_count[tags]\n",
        "\n",
        "  tags_transition_prob = tags_transition_count.copy()\n",
        "  for tag in tags:\n",
        "    tags_transition_prob[tag] /= tags_transition_prob[tag].sum()\n",
        "  return tags_transition_count, tags_transition_prob"
      ],
      "metadata": {
        "id": "BLzoXCAuoNxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags_transition_count, tags_transition_prob = find_tags_transition_CountProbTables(train_set)"
      ],
      "metadata": {
        "id": "AfaLJsl0oN0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Algorithm**"
      ],
      "metadata": {
        "id": "Lb7IVRWwqQWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# finding best values\n",
        "def find_best_tag_prob(input_predd, i, word, bestSeq):\n",
        "  tempdata = input_pred.loc[str(i)+'_'+word, :]\n",
        "  bestValue = tempdata.max()\n",
        "  result = dict(tempdata[tempdata == bestValue])\n",
        "  bestSeq[i] = (word, list(result.keys())[0], list(result.values())[0])\n",
        "  return bestSeq"
      ],
      "metadata": {
        "id": "WeDYt9SDtDzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def viterbi_Algorithm(sentence, words, tags, words_emission_prob, tags_transition_prob):\n",
        "  rows = [str(i)+'_'+word for i, word in enumerate(sentence)]\n",
        "  cols = tags\n",
        "  zero_data = np.zeros(shape=(len(rows),len(cols)))\n",
        "  input_pred = pd.DataFrame(zero_data, index=rows, columns=cols)\n",
        "  \n",
        "  bestSeq = {}\n",
        "  # Finding Probabilities for the first word -----------------------------------------\n",
        "  word = sentence[0]\n",
        "  prevTag = \"<S>\"\n",
        "  if word in words:\n",
        "    for tag in tags:\n",
        "      input_pred.loc[str(0)+'_'+word, tag] = words_emission_prob.loc[word, tag]*tags_transition_prob.loc[prevTag, tag]\n",
        "  else:\n",
        "    for tag in tags:\n",
        "      input_pred.loc[str(0)+'_'+word, tag] = tags_transition_prob.loc[prevTag, tag]\n",
        "\n",
        "  # Finding Max value and Best tag for the first word -----------------------------------------\n",
        "  bestSeq = find_best_tag_prob(input_pred, 0, word, bestSeq)\n",
        "\n",
        "  for i in range(1, len(sentence)):\n",
        "    word = sentence[i]\n",
        "    prevTag = bestSeq[i-1][1]\n",
        "    if word in words:\n",
        "      for tag in tags:\n",
        "        input_pred.loc[str(i)+'_'+word, tag] = words_emission_prob.loc[word, tag]*tags_transition_prob.loc[prevTag, tag]\n",
        "    else:\n",
        "      for tag in tags:\n",
        "        input_pred.loc[str(i)+'_'+word, tag] = tags_transition_prob.loc[prevTag, tag]\n",
        "    \n",
        "    # Finding Max value and Best tag for the words -----------------------------------------\n",
        "    bestSeq = find_best_tag_prob(input_pred, i, word, bestSeq)\n",
        "    Y_pred = [bestSeq[i][1] for i in range(len(bestSeq))]\n",
        "    return input_pred, bestSeq, Y_pred\n"
      ],
      "metadata": {
        "id": "58tcPO_sqlxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(Y, Y_pred):\n",
        "  count = 0\n",
        "  for i in range(len(Y_pred)):\n",
        "    if Y[i] == Y_pred[i]:\n",
        "      count += 1\n",
        "  print(f\"Accuracy : {(count/len(Y_pred))*100}%\")"
      ],
      "metadata": {
        "id": "PgYjvvKCvHn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pick Random Sentenc"
      ],
      "metadata": {
        "id": "3A3YL_fSvf59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = test_set.reset_index(drop=True)\n",
        "test_set[test_set[0]==\"<S>\"].index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLf3YJcTq366",
        "outputId": "3b8a4f75-db1b-4af8-fee1-9adba42f2359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Int64Index([   0,   23,   84,  100,  112,  154,  167,  184,  214,  240,  321,\n",
              "             336,  363,  377,  388,  419,  429,  456,  467,  487,  508,  526,\n",
              "             552,  570,  584,  596,  616,  646,  659,  681,  707,  740,  760,\n",
              "             778,  806,  866,  928,  934,  940,  963,  979, 1013, 1019, 1049,\n",
              "            1055, 1077, 1081, 1097, 1116, 1124, 1146, 1154, 1162, 1179, 1219,\n",
              "            1237, 1269, 1278, 1299, 1342, 1386, 1427, 1457, 1470, 1481, 1492,\n",
              "            1512, 1536, 1543, 1553, 1583, 1611, 1627, 1641, 1663, 1688, 1727,\n",
              "            1780, 1795, 1823, 1836, 1844, 1877, 1887, 1903, 1917, 1951, 1972,\n",
              "            1998, 2013, 2023, 2068, 2124, 2133, 2162, 2183, 2200, 2215, 2248,\n",
              "            2297],\n",
              "           dtype='int64')"
            ]
          },
          "metadata": {},
          "execution_count": 372
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "startIndx = 0+1 \n",
        "endIndx = 23\n",
        "test = test_set[startIndx:endIndx]\n",
        "sentence = list(test[0])\n",
        "Y = list(test[1])"
      ],
      "metadata": {
        "id": "4n16QC_Fqshj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_pred, bestSeq, Y_pred = viterbi_Algorithm(sentence, words, tags, words_emission_prob, tags_transition_prob)"
      ],
      "metadata": {
        "id": "lF97BS_OvHqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_accuracy(Y, Y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmK_RcD1vqI0",
        "outputId": "4ffc49ba-9903-49dd-ec9d-4acf9a94a5a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 100.0%\n"
          ]
        }
      ]
    }
  ]
}