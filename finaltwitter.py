# -*- coding: utf-8 -*-
"""finaltwitter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FFrGyWy1b0ZEcV5gNbfe4hFIb2pIUR_c
"""

import pandas as pd

data=pd.read_csv('train.tsv', lineterminator='\n',error_bad_lines=False, sep = '\t',header= None)
#new testset
data_test = pd.read_csv('test.tsv', lineterminator='\n',error_bad_lines=False, sep = '\t',header= None)

data.drop_duplicates(inplace = True)

data['text'] = data[1]
data['sentiment'] = data[0]
data_test['text'] = data_test[1]
data_test['sentiment'] = data_test[0]

import seaborn as sns
sns.countplot(data['sentiment'])

import re
import numpy as np
import string
string.punctuation

### citation: https://github.com/prateekjoshi565/twitter_sentiment_analysis/blob/master/code_sentiment_analysis.ipynb

def remove_user(tweet,pattern):
  r = re.findall(pattern,tweet)
  for i in r:
    tweet = re.sub(i,'',tweet)

  return tweet

"""Data Cleaning"""

# removing @USER from all tweets
data['tweet without user'] = np.vectorize(remove_user)(data['text'],"@[\w]*")
data_test['tweet without user'] = np.vectorize(remove_user)(data_test['text'],"@[\w]*")

pip install emot

### emot library can replace emojis and emoticons with their proper meaning
import pickle
from emot.emo_unicode import UNICODE_EMOJI # For emojis
from emot.emo_unicode import EMOTICONS_EMO # For EMOTICONS

### citaion: https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python

# Function for converting emojis into word
def convert_emojis(text):
    for emot in UNICODE_EMOJI:
        text = text.replace(emot, "_".join(UNICODE_EMOJI[emot].replace(",","").replace(":","").split()))
    return text

data['tweet without user and emojis'] = data['tweet without user'].apply(convert_emojis)

data_test['tweet without user and emojis'] = data_test['tweet without user'].apply(convert_emojis)

#removing all special charectors from all tweets

data['tweet without user,emo,spec char'] = data['tweet without user and emojis'].str.replace('^A-Za-z1-9#',' ')
data_test['tweet without user,emo,spec char'] = data_test['tweet without user and emojis'].str.replace('^A-Za-z1-9#',' ')



import string
string.punctuation

### removing all punctuations from all tweets

def remove_punc(test):
  message_punc_removed = [i  for i in test if i not in string.punctuation] 
  test_join = ''.join(message_punc_removed)
  return test_join

data['tweet without user,emo,spec char,punc'] = data['tweet without user,emo,spec char'].apply(remove_punc)
data_test['tweet without user,emo,spec char,punc'] = data_test['tweet without user,emo,spec char'].apply(remove_punc)

vocab = remove_punc(data['tweet without user,emo,spec char,punc'])

vocab

words = vocab.rsplit(" ")

words

number_words = len(list(set(words)))
number_words

type(words)

import operator

### citation: https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value

freq_words = dict(zip(*np.unique(words, return_counts=True)))
sorted_d = dict( sorted(freq_words.items(), key=operator.itemgetter(1),reverse=True))

sorted_d

freq_words_stop = {'yn': 46006,
 'i': 26021,
 '': 19308,
 'a': 16631,
 'y': 16048,
 'o': 15981,
 'ar': 12520,
 'fi': 10701,
 'am': 10618,
 'di': 8776,
 'ddim': 6036,
 'bod': 5759,
 'ma': 5747,
 'ti': 5698,
 'mynd': 5512,
 'dwi': 5456,
 'ond': 5256,
 'neud': 4211,
 'na': 4200,
 'I': 4122,
 '{URL}': 3829,
 'ni': 3768,
 'wedi': 3513,
 '-': 3495,
 'yr': 3378,
 'iawn': 3280,
 'dim': 3216,
 'un': 3172,
 'chdi': 3143,
 'fod': 2999,
 'da': 2824,
 'fo': 2658,
 'efo': 2618,
 'dydd': 2475,
 'x': 2439,
 'meddwl': 2383,
 'mor': 2341,
 'diolch': 2309,
 'dwin': 2308,
 'fy': 2301,
 'mewn': 2236,
 'os': 2209,
 'be': 2200,
 "dwi'n": 2183,
 'bo': 2175,
 'dod': 2167,
 'haha': 2121,
 'weld': 2068,
 'cal': 2040,
 'fel': 2016,
 'gweld': 2000,
 'chi': 1962,
 "ti'n": 1952,
 'dy': 1906,
 'gal': 1890,
 'newydd': 1881,
 'wan': 1835,
 'so': 1834,
 'mae': 1796,
 'methu': 1785,
 'dal': 1763,
 'xx': 1760,
 'just': 1729,
 'cael': 1642,
 'odd': 1633,
 "i'r": 1631,
 'nol': 1616,
 'heb': 1611,
 'gwaith': 1593,
 'edrych': 1590,
 'ei': 1576,
 'bach': 1564,
 'Diolch': 1532,
 'nos': 1527,
 'allan': 1518,
 'nhw': 1516,
 'heddiw': 1504,
 "fi'n": 1491,
 'fory': 1479,
 'heno': 1474,
 'n': 1438,
 'neu': 1435,
 'fynd': 1362,
 'ol': 1358,
 'oedd': 1320,
 'fydd': 1319,
 'bob': 1308,
 'hi': 1304,
 'tan': 1298,
 "i'n": 1290,
 'ac': 1272,
 'yma': 1263,
 'pan': 1248,
 'Dwi': 1230,
 'at': 1225,
 'nai': 1216,
 'hyn': 1202,
 'well': 1193,
 "o'r": 1182,
 'cyn': 1171,
 'lle': 1169,
 'yna': 1164,
 'arall': 1141,
 '!': 1130,
 'xxx': 1093,
 'dda': 1076,
 'gan': 1074,
 'de': 1070,
 'gyda': 1063,
 'wedyn': 1053,
 'gallu': 1050,
 'on': 1033,
 'rhaid': 1028,
 '.': 1001,
 'A': 995,
 'gweithio': 992,
 'to': 987,
 'ers': 981,
 'bydd': 979,
 "sy'n": 979,
 'mwy': 978,
 'pawb': 974,
 'wrth': 956,
 'lot': 950,}

len(freq_words_stop)



"""Support Vector Machine """

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split

count = CountVectorizer(max_features=1500)
input_text = count.fit_transform(data['tweet without user,emo,spec char,punc'])

x_train_cv,x_test_cv,y_train_cv,y_test_cv = train_test_split(input_text,data['sentiment'],test_size=0.3)

from sklearn.svm import SVC
svc_classifier = SVC()
svc_classifier.fit(x_train_cv,y_train_cv)

pred = svc_classifier.predict(x_test_cv)
pred

from sklearn.metrics import roc_auc_score
print(roc_auc_score(y_test_cv,pred))

from sklearn.metrics import classification_report
classification_report(y_test_cv,pred)

### testing on new test set

count = CountVectorizer(max_features=1500)
input_text_test = count.fit_transform(data_test['tweet without user,emo,spec char,punc'])
sentiment_test = data_test['sentiment']
test_pred = svc_classifier.predict(input_text_test)
print(roc_auc_score(sentiment_test,test_pred))

from sklearn.metrics import classification_report
classification_report(y_test_cv,pred)
print(classification_report(sentiment_test,test_pred))

### For LSTM  implementaion we have taken references from a udemy course
### citation: https://www.udemy.com/course/ml-and-python-in-finance-real-cases-and-practical-solutions/

import nltk
import keras
import tensorflow
from nltk.tokenize import word_tokenize
from nltk.tokenize import wordpunct_tokenize

#finding the lengths of all tweets in the dataset
lenths =[]
for i in (data['tweet without user,emo,spec char,punc']):
  k = len(wordpunct_tokenize(i))
  lenths.append(k)

lenths

# finding the maximum length of the tweet in the dataset
maxi = 0
for i in lenths:
  if i> maxi:
    maxi = i
    
maxi

# finding the tweets those are above the length of 35 words
lo = 0
for i in lenths:
  if i>35:
    lo = lo + 1

lo

from sklearn.model_selection import train_test_split
x = data['tweet without user,emo,spec char,punc']
y = data['sentiment']
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.1, random_state=41)

from tensorflow.keras.preprocessing.text import one_hot,Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical

# tokenizing all tweets 
tokenizer = Tokenizer(num_words=number_words)
tokenizer.fit_on_texts(x_train)

train_sequence = tokenizer.texts_to_sequences(x_train)

test_sequence = tokenizer.texts_to_sequences(x_test)

#tokenizing all tweets on new test set

x = data_test['tweet without user,emo,spec char,punc']
y = data_test['sentiment']
tokenizer.fit_on_texts(x)
test_seq = tokenizer.texts_to_sequences(x)

# As there are only 32 tweets are above length of 35 strings,so we are truncating all tweets upto 35 

### truncating train data
trunc_train_sequence = []
for i in train_sequence:
  trunc = i[0:35]
  trunc_train_sequence.append(trunc)

#truncating test data

trunc_test_sequence = []
for i in test_sequence:
  trunc = i[0:35]
  trunc_test_sequence.append(trunc)
    
#trunacating all tweets in new test set
trunc_test_seq = []
for i in test_seq:
  trunca = i[0:35]
  trunc_test_seq.append(trunca)    

### As the neural networks accepts only same size of input, so we are padding all train and test sequences to length 35

from tensorflow.keras.preprocessing.sequence import pad_sequences
padded_train = pad_sequences(trunc_train_sequence,maxlen=35, padding = 'post')
padded_test = pad_sequences(trunc_test_sequence,maxlen=35,padding = 'post')
padded_test_new = pad_sequences(trunc_test_seq,maxlen=35,padding = 'post')

#converting sentiment to categorical value

y_train_cat = to_categorical(y_train,2)
y_test_cat = to_categorical(y_test,2)
y_test_new = to_categorical(y,2)

#LSTM model using keras library

model = Sequential()

model.add(Embedding(number_words, output_dim=128))

model.add(LSTM(256))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(2,activation='softmax'))

model.compile(optimizer = 'adam', loss = 'categorical_crossentropy',metrics = ['accuracy'])
model.summary()

model.fit(padded_train,y_train_cat, batch_size= 32, epochs= 5, validation_split= 0.1)

pred = model.predict(padded_test)

predictions = []
for i in pred:
  predictions.append(np.argmax(i))

original = []
for i in y_test_cat:
  original.append(np.argmax(i))

from sklearn.metrics import accuracy_score
accuracy = accuracy_score(original,predictions)
accuracy

### testing LSTM on new test set
pred_test = model.predict(padded_test_new)
predict = []
for i in pred_test:
  predict.append(np.argmax(i))

original_test = []
for i in y_test_new:
  original_test.append(np.argmax(i))

accuracy_test = accuracy_score(original_test,predict)
accuracy_test

